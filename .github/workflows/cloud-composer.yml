name: Deploy to Compute Engine

on:
  push:
    branches: ml-models-arjun-models
  workflow_dispatch:
    inputs:
      deployment_target:
        description: "Where to deploy Airflow"
        required: true
        default: "compute_engine"
        type: choice
        options:
          - local
          - compute_engine

jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      GCP_PROJECT_ID: primordial-veld-450618-n4
      GCP_LOCATION: us-central1
      ARTIFACT_REGISTRY_NAME: airflow-docker-image
      REPO_FORMAT: docker
      DOCKER_IMAGE_NAME: data-pipeline
      DOCKER_IMAGE_TAG: latest
      VM_NAME: airflow-server
      VM_ZONE: us-central1-a
      MACHINE_TYPE: e2-standard-4
      REMOTE_USER: ubuntu

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r Data_Pipeline/tests/requirements-test.txt

      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$(pwd)/Data_Pipeline:$(pwd)" >> $GITHUB_ENV
          echo "PYTHONPATH set to $(pwd)/Data_Pipeline:$(pwd)"

      # - name: Run Unit Tests
      #   run: |
      #     python -m unittest discover -s Data_Pipeline/tests -p "test*.py"

      - name: Authenticate with Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Set up Google Cloud SDK
        uses: google-github-actions/setup-gcloud@v1

      - name: Update gcloud components
        run: |
          gcloud components update --quiet

      - name: Ensure Artifact Registry
        run: |
          chmod +x scripts/create_or_ensure_artifact_registry.sh
          ./scripts/create_or_ensure_artifact_registry.sh

      # ---------- CONDITIONAL DOCKER BUILD ----------
      # - name: Check if Docker Build is Needed
      #   id: detect-changes
      #   run: |
      #     PROJECT_ID="primordial-veld-450618-n4"
      #     REPO_NAME="airflow-docker-image"
      #     IMAGE_NAME="data-pipeline"
      #     IMAGE_TAG="latest"
      #     LOCATION="us-central1"
      #     FULL_IMAGE_PATH="${LOCATION}-docker.pkg.dev/${PROJECT_ID}/${REPO_NAME}/${IMAGE_NAME}"

      #     echo "🔍 Checking if '${IMAGE_NAME}:${IMAGE_TAG}' exists in Artifact Registry..."

      #     # Get JSON output of images in Artifact Registry
      #     IMAGES_JSON=$(gcloud artifacts docker images list "${FULL_IMAGE_PATH}" --include-tags --format="json")

      #     # Extract matching image name & tag
      #     MATCHING_IMAGE=$(echo "$IMAGES_JSON" | jq -r --arg IMAGE_NAME "$FULL_IMAGE_PATH" '.[] | select(.package==$IMAGE_NAME) | .package')

      #     # Flag to check if a build is required
      #     BUILD_REQUIRED=false

      #     if [[ "$MATCHING_IMAGE" == "$FULL_IMAGE_PATH" ]]; then
      #       echo "✅ Exact match found: '${IMAGE_NAME}:${IMAGE_TAG}'"
      #     else
      #       echo "⚠️ No exact match for '${IMAGE_NAME}:${IMAGE_TAG}'. A new build is required."
      #       BUILD_REQUIRED=true
      #     fi

      #     echo "🔍 Checking if 'Dockerfile' or 'requirements.txt' has changed..."

      #     if [ "$(git rev-list --count HEAD)" -lt 2 ]; then
      #       echo "⚠️ Not enough commit history. Assuming changes."
      #       BUILD_REQUIRED=true
      #     else
      #       if ! git diff --quiet HEAD~1 HEAD -- Data_Pipeline/Dockerfile Data_Pipeline/requirements.txt; then
      #         echo "⚠️ Changes detected in 'Dockerfile' or 'requirements.txt'. A new build is required."
      #         BUILD_REQUIRED=true
      #       fi
      #     fi

      #     # Set output based on build requirement
      #     if [[ "$BUILD_REQUIRED" == "true" ]]; then
      #       echo "build_required=true" >> $GITHUB_OUTPUT
      #     else
      #       echo "build_required=false" >> $GITHUB_OUTPUT
      #     fi

      - name: Check if Docker Build is Needed
        id: detect-changes
        run: |
          chmod +x scripts/check_docker_build.sh
          scripts/check_docker_build.sh

      - name: Configure Docker for Artifact Registry
        run: |
          gcloud auth configure-docker us-central1-docker.pkg.dev --quiet

      # - name: Build & Push data-pipeline Image
      #   if: steps.detect-changes.outputs.build_required == 'true'
      #   run: |
      #     echo "🚀 Docker build triggered (image missing or code changed)."
      #     PROJECT_ID="primordial-veld-450618-n4"
      #     REPO_NAME="airflow-docker-image"
      #     IMAGE_NAME="data-pipeline"
      #     IMAGE_TAG=$(date +'%Y%m%d-%H%M%S')
      #     REPO_URI="us-central1-docker.pkg.dev/${PROJECT_ID}/${REPO_NAME}"

      #     # Build with --no-cache to ensure a fresh build
      #     docker build --no-cache -t ${REPO_URI}/${IMAGE_NAME}:${IMAGE_TAG} -f Data_Pipeline/Dockerfile Data_Pipeline

      #     # Also tag this as 'latest' for convenience
      #     docker tag ${REPO_URI}/${IMAGE_NAME}:${IMAGE_TAG} ${REPO_URI}/${IMAGE_NAME}:latest

      #     # Push both the timestamped tag and the latest tag
      #     docker push ${REPO_URI}/${IMAGE_NAME}:${IMAGE_TAG}
      #     docker push ${REPO_URI}/${IMAGE_NAME}:latest

      #     echo "Image pushed successfully with tag: ${IMAGE_TAG}"

      - name: Build & Push data-pipeline Image
        if: steps.detect-changes.outputs.build_required == 'true'
        run: |
          chmod +x scripts/build_and_push.sh
          scripts/build_and_push.sh

      # - name: Test Docker Push
      #   if: steps.detect-changes.outputs.build_required == 'true'
      #   run: |
      #     # Set variables
      #     PROJECT_ID="primordial-veld-450618-n4"
      #     REPO_NAME="airflow-docker-image"
      #     IMAGE_NAME="test-push-image"
      #     IMAGE_TAG=$(date +'%Y%m%d-%H%M%S')  # Unique tag for each build
      #     REPO_URI="us-central1-docker.pkg.dev/${PROJECT_ID}/${REPO_NAME}"

      #     # Create a minimal Dockerfile for testing
      #     echo -e "FROM alpine\nCMD [\"echo\", \"Test push successful!\"]" > Dockerfile.test

      #     # Build with --no-cache to ensure a fresh build
      #     docker build --no-cache -t ${REPO_URI}/${IMAGE_NAME}:${IMAGE_TAG} -f Dockerfile.test .

      #     # Also tag this as 'latest' for convenience
      #     docker tag ${REPO_URI}/${IMAGE_NAME}:${IMAGE_TAG} ${REPO_URI}/${IMAGE_NAME}:latest

      #     # Push both the timestamped tag and the latest tag
      #     docker push ${REPO_URI}/${IMAGE_NAME}:${IMAGE_TAG}
      #     docker push ${REPO_URI}/${IMAGE_NAME}:latest

      #     echo "Image pushed successfully with tag: ${IMAGE_TAG}"

      - name: Ensure Airflow Server VM & Firewall
        if: ${{ github.event_name == 'push' || inputs.deployment_target == 'compute_engine' || inputs.deployment_target == 'both' }}
        run: |
          chmod +x scripts/create_or_start_airflow_server_vm.sh
          ./scripts/create_or_start_airflow_server_vm.sh

      - name: Determine Compute Engine User
        run: |
          COMPUTE_ENGINE_USER=$(gcloud config get-value account 2>/dev/null || echo "ubuntu")
          echo "COMPUTE_ENGINE_USER=${COMPUTE_ENGINE_USER}" >> $GITHUB_ENV
          echo "✅ COMPUTE_ENGINE_USER set to ${COMPUTE_ENGINE_USER}"

      # ---------- SSH KEYS -----------
      # - name: Configure SSH to VM
      #   run: |
      #     echo "🚀 Ensuring SSH key for GitHub Actions..."
      #     if [ ! -f ~/.ssh/github-actions-key ]; then
      #       ssh-keygen -t rsa -b 4096 -C "github-actions" -N "" -f ~/.ssh/github-actions-key
      #       echo "✅ SSH Key generated!"
      #     else
      #       echo "✅ SSH Key already exists!"
      #     fi

      #     PUBLIC_KEY=$(cat ~/.ssh/github-actions-key.pub)
      #     VM_NAME="airflow-server"
      #     VM_ZONE="us-central1-a"

      #     # Attach public key to VM metadata if missing
      #     EXISTING_KEYS=$(gcloud compute instances describe $VM_NAME --zone $VM_ZONE --format="value(metadata.ssh-keys)" || echo "")
      #     if [[ "$EXISTING_KEYS" != *"$PUBLIC_KEY"* ]]; then
      #       echo "🔑 Adding SSH key to VM metadata..."
      #       gcloud compute instances add-metadata $VM_NAME --zone $VM_ZONE \
      #         --metadata=ssh-keys="${{ secrets.COMPUTE_ENGINE_USER }}:$PUBLIC_KEY"
      #       echo "✅ SSH key added to VM!"
      #     else
      #       echo "✅ SSH key is already in VM metadata!"
      #     fi

      - name: Configure SSH to VM
        run: |
          chmod +x scripts/configure_ssh.sh
          ./scripts/configure_ssh.sh

      # - name: Sync Airflow Project Files to VM
      #   run: |
      #     VM_NAME="airflow-server"
      #     VM_ZONE="us-central1-a"
      #     echo "🚀 Syncing project files to $VM_NAME..."

      #     # Dynamically fetch the external IP of the VM
      #     EXTERNAL_IP=$(gcloud compute instances describe "$VM_NAME" --zone "$VM_ZONE" --format="get(networkInterfaces[0].accessConfigs[0].natIP)")
      #     echo "Fetched external IP: $EXTERNAL_IP"

      #     # Use the fetched external IP for SSH
      #     ssh -o StrictHostKeyChecking=no -i ~/.ssh/github-actions-key ${{ secrets.COMPUTE_ENGINE_USER }}@$EXTERNAL_IP << 'EOF'
      #       sudo mkdir -p /opt/airflow
      #       sudo chown -R $USER:$USER /opt/airflow
      #       sudo chmod -R 775 /opt/airflow
      #     EOF

      #     # Use the fetched external IP for rsync
      #     rsync -avz \
      #       -e "ssh -o StrictHostKeyChecking=no -i ~/.ssh/github-actions-key" \
      #       --exclude '.git' \
      #       . ${{ secrets.COMPUTE_ENGINE_USER }}@$EXTERNAL_IP:/opt/airflow

      #     echo "✅ Files synced successfully."

      - name: Sync Airflow Project Files to VM
        run: |
          chmod +x scripts/sync_files.sh
          ./scripts/sync_files.sh

      - name: Deploy Airflow on Compute Engine
        run: |
          VM_NAME="airflow-server"
          VM_ZONE="us-central1-a"
          echo "🚀 Deploying Airflow on $VM_NAME..."

          # Dynamically fetch the external IP of the VM
          EXTERNAL_IP=$(gcloud compute instances describe "$VM_NAME" --zone "$VM_ZONE" --format="get(networkInterfaces[0].accessConfigs[0].natIP)")
          echo "Fetched external IP: $EXTERNAL_IP"

          # Use the fetched external IP for SSH (Note: using unquoted EOF for variable expansion)
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/github-actions-key ${{ secrets.COMPUTE_ENGINE_USER }}@$EXTERNAL_IP << EOF
            echo "🚀 Ensuring Docker is installed..."
            if ! command -v docker &> /dev/null; then
              echo "❌ Docker is not installed. Installing..."
              sudo apt-get update -y
              echo "🚀 Adding Docker repository..."
              sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common
              curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
              sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu \$(lsb_release -cs) stable"
              sudo apt-get update -y
              sudo apt-get install -y docker-ce docker-ce-cli containerd.io
            else
              echo "✅ Docker is already installed."
            fi

            if ! command -v docker-compose &> /dev/null; then
              echo "❌ Docker Compose not found. Installing latest version..."
              DOCKER_COMPOSE_VERSION=\$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep -Po '"tag_name": "\K.*?(?=")')
              sudo curl -L "https://github.com/docker/compose/releases/download/\${DOCKER_COMPOSE_VERSION}/docker-compose-\$(uname -s)-\$(uname -m)" -o /usr/local/bin/docker-compose
              sudo chmod +x /usr/local/bin/docker-compose
              sudo ln -sf /usr/local/bin/docker-compose /usr/bin/docker-compose
            else
              echo "✅ Docker Compose is already installed."
            fi

            # Give user Docker permissions
            echo "🔄 Adding user to Docker group..."
            sudo usermod -aG docker \$USER
            newgrp docker
            sudo systemctl restart docker
            echo "✅ User added to Docker group and Docker restarted."

            # Fix Docker socket perms
            sudo chmod 666 /var/run/docker.sock
            echo "✅ Docker socket permissions fixed."

            mkdir -p /opt/airflow
            echo "airflow dir created."
            echo "🚀 Ensuring GCP Key File exists..."
            if [ -d /opt/airflow/gcp-key.json ]; then
                echo "⚠️ Found directory at /opt/airflow/gcp-key.json. Removing it..."
                sudo rm -rf /opt/airflow/gcp-key.json
            fi
            echo "🚀 Creating GCP Key File..."
            echo '${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}' | jq . > /opt/airflow/gcp-key.json
            chmod 644 /opt/airflow/gcp-key.json
            sudo chown ubuntu:docker /opt/airflow/gcp-key.json
            echo "✅ GCP Key File Created."

            echo "🚀 Fixing Airflow log directory permissions..."
            sudo mkdir -p /opt/airflow/logs
            sudo chmod -R 777 /opt/airflow/logs
            sudo chown -R \$USER:\$USER /opt/airflow/logs
            
            cd /opt/airflow

            echo "🚀 Pulling the latest image from Artifact Registry..."
            gcloud auth configure-docker us-central1-docker.pkg.dev --quiet
            docker compose pull || true

            echo "🚀 Stopping any running containers..."
            docker compose down || true

            # Remove postgres volume if you want to reset the DB (warning: this clears data)
            docker volume rm airflow_postgres-db-volume || true

            echo "🚀 Starting Airflow using Docker Compose..."
            docker compose up -d --remove-orphans

            echo "✅ Airflow successfully started!"
          EOF

      # - name: Deploy Airflow on Compute Engine
      #   env:
      #     GCP_SERVICE_ACCOUNT_KEY: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
      #   run: |
      #     chmod +x scripts/deploy_airflow.sh
      #     scripts/deploy_airflow.sh

      - name: Get Airflow Webserver IP
        run: |
          VM_NAME="airflow-server"
          VM_ZONE="us-central1-a"
          echo "🚀 Syncing project files to $VM_NAME..."

          # Dynamically fetch the external IP of the VM
          EXTERNAL_IP=$(gcloud compute instances describe "$VM_NAME" --zone "$VM_ZONE" --format="get(networkInterfaces[0].accessConfigs[0].natIP)")
          echo "🌍 Airflow UI is available at: http://$EXTERNAL_IP:8080"

      # ✅ Remove SSH Key after deployment (Security)
      - name: Remove SSH Key
        run: rm -f ~/.ssh/github-actions-key ~/.ssh/github-actions-key.pub
